# AI基础第二次编程作业：统计机器学习

### 李喆昊 PB17050941

## 目录 

1. **有监督学习**
   1. 数据集介绍与统计分析
   2. KNN
      1. 原理介绍
      2. 实现与测试
   3. SVM
      1. 原理介绍
      2. 实现与测试
   4. Logisitic Regression
      1. 原理介绍
      2. 实现与测试
2. **无监督学习**
   1. 数据集介绍
   2. PCA
      1. 原理介绍
      2. 实现与测试
   3. K-means
      1. 原理介绍
      2. 实现与测试
3. **总结与反思**



## 一. 有监督学习

有监督学习是指有数据标签的一类统计机器学习模式。常用方法有KNN、SVM、Logistic Regression、深度神经网络等。

1. **数据集介绍与统计分析**

   本次实验采用的是UCI大学的学生信息及在校表现数据集， 提供了数学和葡萄牙语两个科目的数据，包含学生成绩、家庭背景、生活习惯等共33个属性。数学数据集和葡萄牙语数据集分别有395条数据和649条数据，

   数据属性中有些取值为字符串，如学校，性别，父母职业等，另外一些取值为数值，如学习时间，在校成绩等。我们的目标是预测学生的在校最终成绩G3。为了选取最合适的特征进行学习，我首先分析了取值为数值的属性与G3的相关性关系如下图所示。

   数学数据集：

   ![cor-math](/Users/bytedance/Desktop/AI/report/images/cor-math.png)

   葡萄牙语数据集：

   ![cor-por](/Users/bytedance/Desktop/AI/report/images/cor-por.png)

   可以看到：在两个数据集中，G1，G2与G3有着非常强的正相关关系。Medu，Fedu代表的父母教育程度也对孩子的最终成绩有着较大的正相关关系。有趣的是，学习时间与葡萄牙语最终成绩正相关性较强（0.25），但与数学最终成绩正相关性较弱（0.098）。除此之外，在两个数据集中，failure属性代表的学生在课堂中的失败次数与G3有着较强的负相关关系。年龄、旅行时间与G3也有一定的负相关关系。

   对于取值不为数值的属性，我使用``sklearn.preprocessing.LabelEncoder()``将其转换为数值，作出相关性矩阵如下：

   数学数据集：

   ![math2](/Users/bytedance/Desktop/AI/report/images/math2.png)

   葡萄牙语数据集：

   ![por2](/Users/bytedance/Desktop/AI/report/images/por2.png)

   从结果可以看到，葡萄牙语数据集中，学校和G3有种较强的负相关关系，higer属性代表的“是否想进入高等学校学习”与葡萄牙语G3成绩有种较强的正相关关系。除此之外，在两个数据集中，性别、住址、母亲的工作、进入学校的原因、是否有网络都与G3有一定的相关关系。

   根据上面的分析结果，我们可以制定特征选取的策略如下：

   1. 选取G1，G2预测G3  
   2. 选取除G1，G2外的其他所有特征预测G3

   3. 选取与G3相关性较强的Medu，Fedu，failures，age，traverltime，sex，address，Mjob，reason，internet，higher预测G3

   通过对数据进行其他统计分析后发现没有离群值，不需要进行数据删减。

   对数据集进行7:3划分为训练集与测试集。

   | 数据集   | 数据总数 | 训练集大小 | 测试集大小 |
   | -------- | -------- | ---------- | ---------- |
   | 数学     |          |            |            |
   | 葡萄牙语 |          |            |            |

   

1. **KNN**

   1. 原理介绍

      KNN是K Nearest Neighbor的缩写，正所谓“物以类聚”，KNN算法的核心思想是：同一类数据样本在特征空间中的分布位置是相近的。因此，对于一个未知类别的测试样本，我们可以通过计算与其在特征空间中距离最近的K的点，以这K个点中占比最多的类型作为测试样本的类型。

   2. 实现方式与测试结果

      实现KNN的关键问题是：如何计算测试样本与其他所有已分类样本的距离（这里使用的是欧式距离），并选取最近的K个样本的标签进行投票。

      计算距离可以使用``numpy``的broadcast机制，核心代码如下：

      ```python
   distance = ((testdata - dataset)**2).sum(axis = 1)
      idx = np.argsort(distance)
   idx = idx[:K] # select the cloest K points 
      ```
      
      投票：
      
      ```python
   predict_label = 1 if train_label[knn_idx].sum() > K / 2 else 0
      ```
      
      测试结果：
      
      选取的参考点点数量K对结果的影响
      
      | K    | F1   | Precision | Recall |
      | ---- | ---- | --------- | ------ |
      | 1    |      |           |        |
      | 3    |      |           |        |
      | 5    |      |           |        |
      | 9    |      |           |        |
      | 15   |      |           |        |
      
      从结果可以看到：
      
      

2. **SVM**

   1. 原理介绍

      SVM是Support Vector Machine的缩写。SVM的核心思想是寻找能够将数据样本进行分类的超平面，并使得该超平面到被其分隔开的两类样本的最小距离最大化。与最终得到的超平面取得最小距离的那些数据样本被称为“支持向量”，而被划分开的两类样本的支持向量到超平面的最小距离之和称为“Margin”。

      首先我们考察只能对线性可分数据进行划分的支持向量机，又被称为“Hard Margin SVM”，其算法过程可以被概括为一个最优化问题：
      $$
      \min_{w,b} \frac{1}{\|w\|} \\ s.t. \ 1 - y_i(w^Tx_i + b) \leq 0 \\ y_i \in \{-1,1\}
      $$
      为了求解这一问题，我们将原问题改写一下，并使用Lagrangian乘子法。
      $$
      \max_{w,b} \frac{1}{2}\|w\|^2 \\ s.t. \ 1 - y_i(wx_i + b) \leq 0
      $$

      $$
      L(\alpha,w,b) =  \frac{1}{2}\|w\|^2 + \sum_{i = 1}^n\alpha_i(1 - y_i(wx_i + b) ), \ \alpha_i \geq 0 \tag{1}
      $$

      求解$(1)$并整理后得：

      

      // 介绍SVM的原理：原始形式，对偶问题。因为推导不是重点所以直接放推导结果。

      // 介绍SVM的几种kernel以及kernel的意义：是不同的度量方式，将特征映射到不同的特征空间，从而使得原来线性不可分的样本在新的特征空间中变得线性可分。// 放图

   2. 实现方式与测试结果

      // SVM的实现：代码，使用了二次规划库

      // 结果：

      C = xxx

      

      | 特征 | 线性核 | 多项式 | rbf  |
      | ---- | ------ | ------ | ---- |
      |      |        |        |      |
      |      |        |        |      |
      |      |        |        |      |

      RBF 调参 // 要介绍不同参数的作用，放图

      | C    | $\gamma$ | F1   |
      | ---- | -------- | ---- |
      |      |          |      |
      |      |          |      |
      |      |          |      |

      poly调参：

      |      |      |      |
      | ---- | ---- | ---- |
      |      |      |      |
      |      |      |      |
      |      |      |      |

      

## 二. 无监督学习

1. 数据集介绍

   // 数据集来源和数量

2. PCA

   1. 原理介绍

      // PCA本质：协方差矩阵的特征值分解：特征值为什么重要？？特征值最大的特征向量的方向是方差最大的方向，其余同理。推导过程只放结果

   2. 实现方式与测试结果

      // 测试结果：要放特征值的柱状图，显示百分比

3. K-means

   1. 原理介绍

      // k-means：本质和knn一样，通过特征空间的位置分布将数据聚类

      // 介绍兰德系数和轮廓系数，怎么计算，什么意义（why，what，how）

   2. 实现方式与测试结果

      //放2维时的聚类图

      //放threshold的测试表格

      //放聚类数量K的测试表格

## 三. 总结与反思

1. 总结：学到很多，数据分析，各类算法的实现，python的简洁
2. 反思：细节上花时间


{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_knn_search(dataset, testdata, K):\n",
    "    '''\n",
    "        Funtion: Find the K nearest neighbor of testdata in dataset \n",
    "        Param: \n",
    "            dataset: train/test dataset\n",
    "            testdata: single data vector \n",
    "            K: knn param\n",
    "        Return: the idx of K results in dataset. \n",
    "    '''\n",
    "    ndata = dataset.shape[0]\n",
    "    K = K if K < ndata else ndata\n",
    "    distance = ((testdata - dataset)**2).sum(axis = 1)\n",
    "    idx = np.argsort(distance)\n",
    "    idx = idx[:K] # select the cloest K points \n",
    "    return idx \n",
    "\n",
    "def knn_predict(train_data, train_label, test_data, K):\n",
    "    '''\n",
    "        Function: predict label with knn \n",
    "        Param: \n",
    "            train_data: train dataset \n",
    "            test_data: test dataset \n",
    "            K: cluster num \n",
    "        Return: prediction label of testdata\n",
    "    '''\n",
    "    knn_predict_label = []\n",
    "    n_test_data = test_data.shape[0]\n",
    "    for i in range(n_test_data):\n",
    "        knn_idx = quadratic_knn_search(train_data, test_data[i], K)\n",
    "        predict_label = 1 if train_label[knn_idx].sum() > K / 2 else 0\n",
    "        knn_predict_label.append(predict_label) \n",
    "    return knn_predict_label\n",
    "\n",
    "\n",
    "def test_knn(train_data, train_label, test_data, test_label, K):\n",
    "    t0 = clock()\n",
    "    predict_label = knn_predict(train_data, train_label, test_data, K)\n",
    "    F1, P, R = calc_F1_score(predict_label, test_label)\n",
    "    t1 = clock()\n",
    "    \n",
    "    print(f\"======= KNN: K = {K} ====== \")\n",
    "    print(f\"F1 score: {F1}, Precision: {P}, Recall: {R}\")\n",
    "    print(f\"time used: {t1 - t0}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.63157895 0.68421053]\n [0.68421053 0.68421053]\n [0.52631579 0.57894737]\n [0.47368421 0.42105263]\n [0.26315789 0.26315789]\n [0.73684211 0.73684211]\n [0.42105263 0.42105263]\n [0.31578947 0.26315789]\n [0.42105263 0.42105263]\n [0.52631579 0.47368421]\n [0.42105263 0.36842105]\n [0.42105263 0.47368421]\n [0.63157895 0.        ]\n [0.68421053 0.63157895]\n [0.68421053 0.73684211]\n [0.68421053 0.68421053]\n [0.84210526 0.84210526]\n [0.36842105 0.31578947]\n [0.63157895 0.57894737]\n [0.36842105 0.52631579]\n [0.57894737 0.57894737]\n [0.36842105 0.36842105]\n [0.26315789 0.26315789]\n [0.52631579 0.63157895]\n [0.73684211 0.78947368]\n [0.52631579 0.        ]\n [0.57894737 0.68421053]\n [0.47368421 0.42105263]\n [0.52631579 0.52631579]\n [0.52631579 0.47368421]\n [0.63157895 0.68421053]\n [0.42105263 0.47368421]\n [0.47368421 0.47368421]\n [0.42105263 0.52631579]\n [0.52631579 0.47368421]\n [0.68421053 0.68421053]\n [0.36842105 0.36842105]\n [0.63157895 0.57894737]\n [0.63157895 0.63157895]\n [0.78947368 0.78947368]\n [0.47368421 0.52631579]\n [0.36842105 0.31578947]\n [0.57894737 0.57894737]\n [0.42105263 0.42105263]\n [0.42105263 0.        ]\n [0.36842105 0.47368421]\n [0.57894737 0.63157895]\n [0.73684211 0.68421053]\n [0.57894737 0.57894737]\n [0.52631579 0.57894737]\n [0.52631579 0.47368421]\n [0.89473684 0.89473684]\n [0.47368421 0.52631579]\n [0.94736842 1.        ]\n [0.57894737 0.57894737]\n [0.52631579 0.47368421]\n [0.42105263 0.42105263]\n [0.42105263 0.42105263]\n [0.52631579 0.68421053]\n [0.73684211 0.68421053]\n [0.89473684 0.78947368]\n [0.63157895 0.73684211]\n [0.52631579 0.52631579]\n [0.63157895 0.63157895]\n [0.36842105 0.42105263]\n [0.78947368 0.78947368]\n [0.31578947 0.52631579]\n [0.78947368 0.73684211]\n [0.63157895 0.68421053]\n [0.57894737 0.68421053]\n [0.84210526 0.89473684]\n [0.84210526 0.94736842]\n [0.84210526 0.84210526]\n [0.57894737 0.68421053]\n [0.52631579 0.42105263]\n [0.36842105 0.36842105]\n [0.42105263 0.57894737]\n [0.63157895 0.68421053]\n [0.36842105 0.42105263]\n [0.57894737 0.47368421]\n [0.42105263 0.47368421]\n [0.89473684 0.84210526]\n [0.42105263 0.42105263]\n [0.84210526 0.89473684]\n [0.42105263 0.47368421]\n [0.26315789 0.        ]\n [0.73684211 0.63157895]\n [0.42105263 0.42105263]\n [0.47368421 0.52631579]\n [0.42105263 0.52631579]\n [0.68421053 0.68421053]\n [0.68421053 0.68421053]\n [0.52631579 0.52631579]\n [0.68421053 0.78947368]\n [0.47368421 0.47368421]\n [0.68421053 0.57894737]\n [0.94736842 0.84210526]\n [0.36842105 0.47368421]\n [0.57894737 0.52631579]\n [0.84210526 0.63157895]\n [0.42105263 0.36842105]\n [0.42105263 0.47368421]\n [0.78947368 0.73684211]\n [0.68421053 0.68421053]\n [0.89473684 0.94736842]\n [0.57894737 0.57894737]\n [0.57894737 0.57894737]\n [0.47368421 0.47368421]\n [0.73684211 0.78947368]\n [0.68421053 0.73684211]\n [0.31578947 0.36842105]\n [0.68421053 0.68421053]\n [0.31578947 0.26315789]\n [0.52631579 0.52631579]\n [0.84210526 0.84210526]\n [0.78947368 0.84210526]\n [0.73684211 0.68421053]\n [0.89473684 0.84210526]\n [0.52631579 0.52631579]\n [0.57894737 0.        ]\n [0.47368421 0.52631579]\n [0.36842105 0.31578947]\n [0.36842105 0.31578947]\n [0.63157895 0.63157895]\n [0.57894737 0.47368421]\n [0.84210526 0.78947368]\n [0.47368421 0.42105263]\n [0.57894737 0.57894737]\n [0.78947368 0.68421053]\n [0.42105263 0.52631579]\n [0.52631579 0.57894737]\n [0.68421053 0.57894737]\n [0.52631579 0.47368421]\n [0.57894737 0.52631579]\n [0.63157895 0.52631579]\n [0.63157895 0.63157895]\n [0.47368421 0.63157895]\n [0.68421053 0.63157895]\n [0.31578947 0.26315789]\n [0.47368421 0.47368421]\n [0.78947368 0.73684211]\n [0.36842105 0.52631579]\n [0.47368421 0.42105263]\n [0.42105263 0.36842105]\n [0.42105263 0.36842105]\n [0.47368421 0.57894737]\n [0.78947368 0.84210526]\n [0.63157895 0.63157895]\n [0.36842105 0.        ]\n [0.31578947 0.42105263]\n [0.52631579 0.52631579]\n [0.73684211 0.84210526]\n [0.36842105 0.31578947]\n [0.31578947 0.31578947]\n [0.63157895 0.63157895]\n [0.84210526 0.94736842]\n [0.84210526 0.78947368]\n [0.63157895 0.63157895]\n [0.68421053 0.68421053]\n [0.52631579 0.52631579]\n [0.31578947 0.26315789]\n [0.94736842 1.        ]\n [0.52631579 0.52631579]\n [0.68421053 0.68421053]\n [0.57894737 0.68421053]\n [0.47368421 0.47368421]\n [0.84210526 0.73684211]\n [0.42105263 0.47368421]\n [0.57894737 0.57894737]\n [0.73684211 0.73684211]\n [0.78947368 0.84210526]\n [0.31578947 0.26315789]\n [0.68421053 0.57894737]\n [0.73684211 0.63157895]\n [0.57894737 0.63157895]\n [0.84210526 0.78947368]\n [0.63157895 0.63157895]\n [0.73684211 0.63157895]\n [0.36842105 0.21052632]\n [0.36842105 0.42105263]\n [0.57894737 0.47368421]\n [0.78947368 0.78947368]\n [0.42105263 0.52631579]\n [0.57894737 0.63157895]\n [0.42105263 0.42105263]\n [0.84210526 0.78947368]\n [0.52631579 0.63157895]\n [0.73684211 0.73684211]\n [0.73684211 0.84210526]\n [0.52631579 0.42105263]\n [0.94736842 0.94736842]\n [0.47368421 0.36842105]\n [0.84210526 0.78947368]\n [0.94736842 0.94736842]\n [0.26315789 0.        ]\n [0.73684211 0.78947368]\n [0.52631579 0.42105263]\n [0.26315789 0.47368421]\n [0.52631579 0.52631579]\n [0.63157895 0.78947368]\n [0.36842105 0.52631579]\n [0.31578947 0.31578947]\n [0.84210526 0.78947368]\n [0.42105263 0.42105263]\n [0.63157895 0.68421053]\n [0.63157895 0.52631579]\n [0.36842105 0.31578947]\n [0.26315789 0.42105263]\n [0.36842105 0.36842105]\n [0.68421053 0.68421053]\n [0.42105263 0.36842105]\n [0.78947368 0.73684211]\n [0.73684211 0.78947368]\n [0.47368421 0.52631579]\n [0.42105263 0.47368421]\n [0.52631579 0.68421053]\n [0.36842105 0.42105263]\n [0.47368421 0.47368421]\n [0.78947368 0.84210526]\n [0.68421053 0.73684211]\n [0.52631579 0.68421053]\n [0.52631579 0.47368421]\n [0.52631579 0.68421053]\n [0.63157895 0.57894737]\n [0.52631579 0.57894737]\n [0.63157895 0.63157895]\n [0.52631579 0.52631579]\n [0.57894737 0.57894737]\n [0.36842105 0.47368421]\n [0.52631579 0.68421053]\n [0.68421053 0.63157895]\n [0.84210526 0.78947368]\n [0.84210526 0.78947368]\n [0.47368421 0.47368421]\n [0.47368421 0.42105263]\n [0.73684211 0.63157895]\n [0.36842105 0.36842105]\n [0.31578947 0.36842105]\n [0.63157895 0.63157895]\n [0.63157895 0.68421053]\n [0.78947368 0.78947368]\n [0.31578947 0.36842105]\n [0.36842105 0.36842105]\n [0.57894737 0.57894737]\n [0.42105263 0.47368421]\n [0.78947368 0.68421053]\n [0.73684211 0.73684211]\n [0.52631579 0.52631579]\n [0.47368421 0.47368421]\n [0.36842105 0.47368421]\n [0.73684211 0.57894737]\n [0.84210526 0.89473684]\n [0.36842105 0.52631579]\n [0.36842105 0.42105263]\n [0.52631579 0.47368421]\n [0.63157895 0.63157895]\n [0.47368421 0.47368421]\n [0.63157895 0.52631579]\n [0.36842105 0.        ]\n [0.78947368 0.78947368]\n [0.63157895 0.73684211]\n [0.63157895 0.57894737]\n [0.84210526 0.94736842]\n [0.26315789 0.31578947]\n [0.68421053 0.68421053]\n [0.68421053 0.78947368]\n [0.57894737 0.52631579]\n [0.78947368 0.73684211]\n [0.36842105 0.        ]\n [0.94736842 0.94736842]\n [0.78947368 0.78947368]\n [0.42105263 0.47368421]\n [1.         0.94736842]\n [0.89473684 0.89473684]\n [0.31578947 0.31578947]\n [0.47368421 0.47368421]]\n"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from time import clock\n",
    "\n",
    "def calc_F1_score(predict_label, truth_label):\n",
    "    '''\n",
    "        Function: calculate F1 score \n",
    "        Param: predict_label, truth_label\n",
    "        Return: list: [F1, Precision, Recall] \n",
    "\n",
    "    '''\n",
    "    compare = predict_label - truth_label \n",
    "    FP = len(compare[compare == 1]) # predict = 1, truth = 0\n",
    "    FN = len(compare[compare == -1]) # predict = 0, truth = 1\n",
    "    \n",
    "    compare = predict_label + truth_label\n",
    "    TP = len(compare[compare == 2]) # predict = 1, truth = 1\n",
    "    TP = len(compare[compare == 0]) # predict = 0, truth = 0\n",
    "    P = TP / (TP + FP) # precision\n",
    "    R = TP / (TP + FN) # recall\n",
    "    F1 = 2 * P * R / (P + R)\n",
    "    return [F1, P, R] \n",
    "\n",
    "def main():\n",
    "    math_data = pd.read_csv('data/student/student-mat.csv', sep=';')\n",
    "\n",
    "    # select feature \n",
    "    feature_name = ['G1','G2','G3']\n",
    "    data = math_data[feature_name]\n",
    "\n",
    "    # G3 == 1: pass, G3 == 0: fail\n",
    "    data.G3[data.G3 < 10] = 0\n",
    "    data.G3[data.G3 >= 10] = 1\n",
    "    # other preprocessing here  #TODO\n",
    "\n",
    "    # train data and test data partition\n",
    "    train_data = data.sample(frac = 0.7, axis = 0)\n",
    "    test_data = data[~data.index.isin(train_data.index)]\n",
    "\n",
    "    train_label = train_data['G3'].to_numpy()\n",
    "    test_label = test_data['G3'].to_numpy()\n",
    "\n",
    "    del train_data['G3']\n",
    "    del test_data['G3']\n",
    "\n",
    "    train_data = train_data / train_data.max(axis = 0)\n",
    "    train_data = train_data.to_numpy()\n",
    "    test_data = test_data.to_numpy()\n",
    "\n",
    "    print(train_data)\n",
    "\n",
    "    # test_knn(train_data, train_label, test_data, test_label, 1)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bit477194762cfa4ad198d3965fa085f933",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
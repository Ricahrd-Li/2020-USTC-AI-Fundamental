{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_knn_search(dataset, testdata, K):\n",
    "    '''\n",
    "        Funtion: Find the K nearest neighbor of testdata in dataset \n",
    "        Param: \n",
    "            dataset: train/test dataset\n",
    "            testdata: single data vector \n",
    "            K: knn param\n",
    "        Return: the idx of K results in dataset. \n",
    "    '''\n",
    "    ndata = dataset.shape[0]\n",
    "    K = K if K < ndata else ndata\n",
    "    distance = ((testdata - dataset)**2).sum(axis = 1)\n",
    "    idx = np.argsort(distance)\n",
    "    idx = idx[:K] # select the cloest K points \n",
    "    return idx \n",
    "\n",
    "def knn_predict(train_data, train_label, test_data, K):\n",
    "    '''\n",
    "        Function: predict label with knn \n",
    "        Param: \n",
    "            train_data: train dataset \n",
    "            test_data: test dataset \n",
    "            K: cluster num \n",
    "        Return: prediction label of testdata\n",
    "    '''\n",
    "    knn_predict_label = []\n",
    "    n_test_data = test_data.shape[0]\n",
    "    for i in range(n_test_data):\n",
    "        knn_idx = quadratic_knn_search(train_data, test_data[i], K)\n",
    "        predict_label = 1 if train_label[knn_idx].sum() > K / 2 else 0\n",
    "        knn_predict_label.append(predict_label) \n",
    "    return knn_predict_label\n",
    "\n",
    "\n",
    "def test_knn(train_data, train_label, test_data, test_label, K):\n",
    "    t0 = clock()\n",
    "    predict_label = knn_predict(train_data, train_label, test_data, K)\n",
    "    F1, P, R = calc_F1_score(predict_label, test_label)\n",
    "    t1 = clock()\n",
    "    \n",
    "    print(f\"======= KNN: K = {K} ====== \")\n",
    "    print(f\"F1 score: {F1}, Precision: {P}, Recall: {R}\")\n",
    "    print(f\"time used: {t1 - t0}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Medu  G1  G2\n297     4   6   5\n16      4   9  11\n202     1   5   6\n214     4   4   7\n242     4   2   0\n..    ...  ..  ..\n245     2  14  15\n220     2   2   3\n96      4   7  12\n342     3  12  12\n181     3   8  10\n\n[276 rows x 3 columns]\n[[0.         0.         0.        ]\n [1.         0.6        0.6875    ]\n [0.25       0.33333333 0.375     ]\n [1.         0.26666667 0.4375    ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.75       0.6        0.75      ]\n [1.         0.33333333 0.5       ]\n [1.         0.8        0.875     ]\n [0.         0.         0.        ]\n [0.25       0.46666667 0.5625    ]\n [0.         0.         0.        ]\n [0.75       0.53333333 0.5625    ]\n [0.5        0.4        0.4375    ]\n [0.5        0.53333333 0.625     ]\n [0.         0.         0.        ]\n [1.         0.66666667 0.5       ]\n [0.5        0.8        0.8125    ]\n [0.5        0.33333333 0.4375    ]\n [1.         0.6        0.6875    ]\n [0.         0.         0.        ]\n [0.5        0.53333333 0.5625    ]\n [0.         0.         0.        ]\n [0.75       0.86666667 0.875     ]\n [0.         0.         0.        ]\n [0.5        0.26666667 0.4375    ]\n [1.         0.46666667 0.5       ]\n [1.         0.73333333 0.75      ]\n [1.         0.73333333 0.6875    ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [1.         0.8        0.75      ]\n [1.         0.73333333 0.6875    ]\n [0.         0.         0.        ]\n [0.75       0.53333333 0.5625    ]\n [0.25       0.53333333 0.5625    ]\n [0.25       0.46666667 0.5       ]\n [1.         0.8        0.875     ]\n [0.         0.         0.        ]\n [0.75       0.6        0.4375    ]\n [0.         0.         0.        ]\n [0.5        0.4        0.5625    ]\n [0.5        0.46666667 0.5       ]\n [1.         0.73333333 0.5625    ]\n [0.75       0.6        0.5625    ]\n [0.25       0.66666667 0.6875    ]\n [0.         0.         0.        ]\n [1.         0.53333333 0.4375    ]\n [1.         0.53333333 0.5       ]\n [0.         0.         0.        ]\n [1.         0.66666667 0.6875    ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.5        0.53333333 0.4375    ]\n [0.75       0.8        0.8125    ]\n [0.25       0.4        0.4375    ]\n [0.         0.         0.        ]\n [1.         0.66666667 0.75      ]\n [0.5        0.53333333 0.625     ]\n [0.         0.6        0.75      ]\n [0.75       0.66666667 0.75      ]\n [0.         0.         0.        ]\n [0.75       0.4        0.625     ]\n [0.25       0.4        0.4375    ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.5        0.53333333 0.5625    ]\n [0.75       0.53333333 0.5       ]\n [1.         0.33333333 0.5       ]\n [0.5        0.46666667 0.4375    ]\n [0.         0.         0.        ]\n [0.5        0.4        0.625     ]\n [1.         0.4        0.4375    ]\n [0.75       0.66666667 0.75      ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.5        0.66666667 0.625     ]\n [0.5        0.53333333 0.625     ]\n [0.75       0.26666667 0.375     ]\n [0.5        0.66666667 0.5625    ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.75       0.93333333 0.9375    ]\n [0.         0.         0.        ]\n [1.         0.93333333 0.9375    ]\n [0.5        0.86666667 0.75      ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [1.         0.53333333 0.75      ]\n [0.75       0.8        0.75      ]\n [0.25       0.6        0.5       ]\n [0.5        0.46666667 0.4375    ]\n [0.5        0.26666667 0.5625    ]\n [1.         0.6        0.5       ]\n [1.         0.73333333 0.8125    ]\n [0.25       0.26666667 0.375     ]\n [1.         0.33333333 0.375     ]\n [0.75       0.46666667 0.375     ]\n [0.5        0.53333333 0.5       ]\n [0.75       0.4        0.4375    ]\n [0.25       0.66666667 0.5625    ]\n [0.5        0.73333333 0.6875    ]\n [0.         0.         0.        ]\n [0.25       0.2        0.3125    ]\n [0.         0.         0.        ]\n [0.25       0.73333333 0.6875    ]\n [0.75       0.46666667 0.5       ]\n [0.         0.         0.        ]\n [1.         0.66666667 0.625     ]\n [1.         0.93333333 1.        ]\n [1.         0.8        0.75      ]\n [0.5        0.8        0.875     ]\n [0.         0.         0.        ]\n [0.5        0.26666667 0.375     ]\n [0.         0.         0.        ]\n [0.75       0.73333333 0.6875    ]\n [1.         0.46666667 0.5       ]\n [0.75       0.73333333 0.5625    ]\n [0.         0.         0.        ]\n [0.5        0.6        0.5       ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [1.         0.73333333 0.8125    ]\n [0.75       0.46666667 0.5625    ]\n [0.         0.         0.        ]\n [1.         0.8        0.75      ]\n [0.         0.         0.        ]\n [1.         0.46666667 0.5       ]\n [0.75       0.46666667 0.5625    ]\n [1.         0.4        0.375     ]\n [0.5        0.73333333 0.75      ]\n [1.         0.4        0.5       ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.75       0.13333333 0.4375    ]\n [0.5        0.93333333 0.9375    ]\n [1.         0.4        0.5625    ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.5        0.53333333 0.625     ]\n [0.         0.         0.        ]\n [0.25       0.6        0.5625    ]\n [0.75       0.66666667 0.625     ]\n [0.         0.         0.        ]\n [0.5        0.46666667 0.5625    ]\n [0.5        0.26666667 0.3125    ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [1.         0.33333333 0.5625    ]\n [0.         0.         0.        ]\n [0.25       0.6        0.5625    ]\n [1.         1.         0.9375    ]\n [0.         0.         0.        ]\n [0.25       0.93333333 0.8125    ]\n [0.5        0.26666667 0.3125    ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.25       0.26666667 0.5       ]\n [1.         0.8        0.75      ]\n [1.         0.86666667 0.9375    ]\n [1.         0.46666667 0.5625    ]\n [0.75       0.53333333 0.5625    ]\n [0.         0.         0.        ]\n [0.5        0.26666667 0.375     ]\n [1.         0.73333333 0.75      ]\n [1.         0.66666667 0.5625    ]\n [1.         0.86666667 0.8125    ]\n [0.5        0.66666667 0.75      ]\n [1.         0.4        0.5       ]\n [0.5        0.4        0.375     ]\n [0.         0.         0.        ]\n [0.25       0.4        0.4375    ]\n [0.5        0.66666667 0.8125    ]\n [0.5        0.4        0.375     ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.75       0.6        0.625     ]\n [0.         0.         0.        ]\n [0.75       0.73333333 0.75      ]\n [0.25       0.4        0.5       ]\n [1.         0.53333333 0.4375    ]\n [0.5        0.53333333 0.5625    ]\n [1.         0.53333333 0.5625    ]\n [0.         0.         0.        ]\n [1.         0.6        0.5       ]\n [0.5        0.46666667 0.625     ]\n [0.         0.         0.        ]\n [0.75       0.66666667 0.625     ]\n [0.5        0.46666667 0.4375    ]\n [1.         0.26666667 0.375     ]\n [0.         0.         0.        ]\n [0.5        0.4        0.5625    ]\n [1.         0.6        0.625     ]\n [0.5        0.46666667 0.4375    ]\n [0.75       0.8        0.9375    ]\n [0.75       0.8        0.75      ]\n [1.         0.8        0.8125    ]\n [1.         0.66666667 0.5625    ]\n [0.25       0.53333333 0.5       ]\n [0.25       0.8        0.75      ]\n [1.         0.53333333 0.5625    ]\n [1.         0.73333333 0.75      ]\n [1.         0.26666667 0.4375    ]\n [0.25       0.53333333 0.4375    ]\n [0.         0.         0.        ]\n [1.         0.46666667 0.625     ]\n [0.         0.         0.        ]\n [0.75       0.6        0.625     ]\n [0.         0.         0.        ]\n [0.25       0.73333333 0.625     ]\n [0.5        0.26666667 0.375     ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [1.         0.4        0.625     ]\n [0.75       0.86666667 0.9375    ]\n [0.75       0.8        0.75      ]\n [1.         0.6        0.625     ]\n [1.         0.73333333 0.6875    ]\n [0.         0.         0.        ]\n [0.5        0.46666667 0.5       ]\n [1.         0.66666667 0.6875    ]\n [0.75       0.66666667 0.5625    ]\n [0.5        0.73333333 0.6875    ]\n [0.         0.         0.        ]\n [0.75       0.2        0.4375    ]\n [1.         0.53333333 0.625     ]\n [0.5        0.46666667 0.5625    ]\n [0.         0.         0.        ]\n [0.         0.8        0.75      ]\n [1.         0.46666667 0.6875    ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.75       0.33333333 0.375     ]\n [1.         0.46666667 0.5       ]\n [0.5        0.53333333 0.5625    ]\n [0.75       0.66666667 0.75      ]\n [0.25       0.26666667 0.3125    ]\n [1.         0.6        0.75      ]\n [0.25       0.33333333 0.375     ]\n [1.         0.26666667 0.4375    ]\n [0.75       0.6        0.6875    ]\n [1.         0.4        0.4375    ]\n [0.75       0.53333333 0.5       ]\n [1.         0.46666667 0.625     ]\n [0.75       0.26666667 0.4375    ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [1.         0.66666667 0.625     ]\n [0.         0.         0.        ]\n [1.         0.4        0.4375    ]\n [0.75       0.46666667 0.5625    ]\n [1.         0.6        0.5625    ]\n [1.         0.66666667 0.75      ]\n [1.         0.4        0.625     ]\n [0.25       0.26666667 0.375     ]\n [0.5        0.46666667 0.4375    ]\n [1.         0.46666667 0.5       ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.         0.         0.        ]\n [0.75       0.4        0.5       ]\n [1.         0.73333333 0.625     ]\n [1.         0.66666667 0.6875    ]\n [0.5        0.93333333 0.9375    ]\n [0.         0.         0.        ]\n [1.         0.46666667 0.75      ]\n [0.75       0.8        0.75      ]\n [0.75       0.53333333 0.625     ]]\n"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from time import clock\n",
    "from sklearn import preprocessing\n",
    "from SVM import *\n",
    "\n",
    "def calc_F1_score(predict_label, truth_label):\n",
    "    '''\n",
    "        Function: calculate F1 score \n",
    "        Param: predict_label, truth_label\n",
    "        Return: list: [F1, Precision, Recall] \n",
    "\n",
    "    '''\n",
    "    compare = predict_label - truth_label \n",
    "    FP = len(compare[compare == 1]) # predict = 1, truth = 0\n",
    "    FN = len(compare[compare == -1]) # predict = 0, truth = 1\n",
    "    \n",
    "    compare = predict_label + truth_label\n",
    "    TP = len(compare[compare == 2]) # predict = 1, truth = 1\n",
    "    TP = len(compare[compare == 0]) # predict = 0, truth = 0\n",
    "    P = TP / (TP + FP) # precision\n",
    "    R = TP / (TP + FN) # recall\n",
    "    F1 = 2 * P * R / (P + R)\n",
    "    return [F1, P, R] \n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    math_data = pd.read_csv('data/student/student-mat.csv', sep=';')\n",
    "\n",
    "    # select feature \n",
    "    feature_name = ['Medu','G1','G2','G3']\n",
    "    data = math_data[feature_name]\n",
    "\n",
    "    # G3 == 1: pass, G3 == 0: fail\n",
    "    data.G3[data.G3 < 10] = 0\n",
    "    data.G3[data.G3 >= 10] = 1\n",
    "    # other preprocessing here  #TODO\n",
    "\n",
    "    # train data and test data partition\n",
    "    train_data = data.sample(frac = 0.7, axis = 0)\n",
    "    test_data = data[~data.index.isin(train_data.index)]\n",
    "\n",
    "    train_label = train_data['G3'].to_numpy()\n",
    "    test_label = test_data['G3'].to_numpy()\n",
    "\n",
    "    del train_data['G3']\n",
    "    del test_data['G3']\n",
    "\n",
    "    train_data = train_data.apply(preprocessing.LabelEncoder().fit_transform)\n",
    "    print(train_data)\n",
    "\n",
    "    # preprocessing.LabelEncoder().fit_transform(train_data)\n",
    "    # preprocessing.LabelEncoder().fit_transform(test_data)\n",
    "\n",
    "    train_data = train_data / train_data.max(axis = 0)\n",
    "    test_data = test_data / test_data.max(axis = 0)\n",
    "    train_data = train_data.to_numpy()\n",
    "    test_data = test_data.to_numpy()\n",
    "\n",
    "    # print(test_data[0:10])\n",
    "\n",
    "    # test_knn(train_data, train_label, test_data, test_label, 1)\n",
    "    def test_add(a,b):\n",
    "        return np.dot(a,b)\n",
    "    \n",
    "    # vadd = np.vectorize(test_add, signature = '(m,n),(n)->(m)')\n",
    "    # result = vadd(test_data, test_data[0])\n",
    "    # print(\"\")\n",
    "    # print(result[0:10])\n",
    "    # print(test_data.T[:10])\n",
    "    ndata = train_data.shape[0]\n",
    "    print(train_data * train_label.reshape((ndata,1)))\n",
    "    # SVM_fit(train_data, train_label, kernel_linear, 100)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SVM:\n",
    "#     def __init__(self, xi, C)\n",
    "import numpy as np\n",
    "import qpsolvers as qs\n",
    "\n",
    "def kernel_linear(x,y):\n",
    "    return np.dot(x,y)\n",
    "\n",
    "def SVM_fit(train_data, train_label, kernel, C):\n",
    "    '''\n",
    "    '''\n",
    "    ndata = train_data.shape[0] # num of samples \n",
    "    vkernel = np.vectorize(kernel, signature='(m,n),(n)->(m)')\n",
    "    P = np.zeros((ndata,ndata))\n",
    "    for i in range(ndata):\n",
    "        P[i] = vkernel(train_data, train_data[i])\n",
    "    print(P)\n",
    "    q = np.ones(ndata)\n",
    "    lb = 0\n",
    "    ub = C\n",
    "    A = train_label\n",
    "    G = A \n",
    "    h = 0\n",
    "    b = 0\n",
    "    alpha = qs.solve_qp(P,q,G,h,A,b,lb,ub)\n",
    "    print(alpha)\n",
    "    return [alpha]\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bit477194762cfa4ad198d3965fa085f933",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}